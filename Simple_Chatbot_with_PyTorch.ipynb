{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This app is a chatbot that uses a deep learning model to respond to user inputs. The program is trained using the data in the ``intents.json'' file and gives appropriate answers to the user. In general, this program consists of several key sections:\n",
        "\n",
        "\n",
        "data: https://www.kaggle.com/datasets/sheetaljade2019/intentsjson\n",
        "\n",
        "\n",
        "\n",
        "1. **Data preprocessing**:\n",
        "    - ``intents.json'' file contains several tags and text patterns (patterns) associated with each tag.\n",
        "    - The data is parsed into text tokens (words) and converted to word roots using a stemmer.\n",
        "    - The input data is displayed as a bag of words.\n",
        "\n",
        "2. Modeling:\n",
        "    - A deep learning model is defined using PyTorch, which includes an LSTM layer and a fully connected layer.\n",
        "    - LSTM model is used to learn input sequences and better model relationships between words.\n",
        "    - The output of the model is a probability distribution for each tag.\n",
        "\n",
        "\n",
        "3. **model training**:\n",
        "    - The model is trained using training data and optimization with Adam's method.\n",
        "    - CrossEntropy cost function is used to evaluate the performance of the model.\n",
        "\n",
        "\n",
        "4. **Prediction and response**:\n",
        "    - User input becomes a bag of words.\n",
        "    - The model predicts the most likely output as a label.\n",
        "    - If the probability of prediction is higher than a certain limit, one of the responses related to that tag is selected from the ``intents.json'' file and displayed to the user.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8VH7Gm4YE_Gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n"
      ],
      "metadata": {
        "id": "kTJlrxSnDT_w"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize the stemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "# Load the intents file\n",
        "with open(\"intents.json\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Load or preprocess the data\n",
        "try:\n",
        "    with open(\"data.pickle\", \"rb\") as f:\n",
        "        words, labels, training, output = pickle.load(f)\n",
        "except:\n",
        "    words = []\n",
        "    labels = []\n",
        "    docs_x = []\n",
        "    docs_y = []\n",
        "\n",
        "    for intent in data[\"intents\"]:\n",
        "        for pattern in intent[\"patterns\"]:\n",
        "            wrds = nltk.word_tokenize(pattern)\n",
        "            words.extend(wrds)\n",
        "            docs_x.append(wrds)\n",
        "            docs_y.append(intent[\"tag\"])\n",
        "\n",
        "        if intent[\"tag\"] not in labels:\n",
        "            labels.append(intent[\"tag\"])\n",
        "\n",
        "    words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
        "    words = sorted(list(set(words)))\n",
        "    labels = sorted(labels)\n",
        "\n",
        "    training = []\n",
        "    output = []\n",
        "\n",
        "    out_empty = [0 for _ in range(len(labels))]\n",
        "\n",
        "    for x, doc in enumerate(docs_x):\n",
        "        bag = []\n",
        "        wrds = [stemmer.stem(w.lower()) for w in doc]\n",
        "\n",
        "        for w in words:\n",
        "            if w in wrds:\n",
        "                bag.append(1)\n",
        "            else:\n",
        "                bag.append(0)\n",
        "\n",
        "        output_row = out_empty[:]\n",
        "        output_row[labels.index(docs_y[x])] = 1\n",
        "\n",
        "        training.append(bag)\n",
        "        output.append(output_row)\n",
        "\n",
        "    training = np.array(training)\n",
        "    output = np.array(output)\n",
        "\n",
        "    with open(\"data.pickle\", \"wb\") as f:\n",
        "        pickle.dump((words, labels, training, output), f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43ELtC9eDubX",
        "outputId": "e3e9a8d2-db9f-45c2-bc76-5f2e1cec5b9c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the PyTorch model with LSTM\n",
        "class ChatModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(ChatModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)\n",
        "        c_0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)\n",
        "        output, (hn, cn) = self.lstm(x, (h_0, c_0))\n",
        "        x = self.fc(output[:, -1, :])\n",
        "        return x\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = len(training[0])\n",
        "hidden_size = 16  # Increased hidden size for better performance\n",
        "output_size = len(output[0])\n",
        "learning_rate = 0.001\n",
        "num_epochs = 1000\n",
        "batch_size = 8\n",
        "\n",
        "# Prepare data\n",
        "train_x = torch.Tensor(training).unsqueeze(1)  # Adding an extra dimension for LSTM\n",
        "train_y = torch.Tensor(output)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = ChatModel(input_size, hidden_size, output_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(0, len(train_x), batch_size):\n",
        "        batch_x = train_x[i:i+batch_size]\n",
        "        batch_y = train_y[i:i+batch_size]\n",
        "\n",
        "        outputs = model(batch_x)\n",
        "        loss = criterion(outputs, torch.max(batch_y, 1)[1])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print(\"Model training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKAD3lw3Dvcv",
        "outputId": "8fe880d7-f96b-4bcb-d5c0-cb93e9cf098f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/1000], Loss: 0.2237\n",
            "Epoch [200/1000], Loss: 0.0251\n",
            "Epoch [300/1000], Loss: 0.0043\n",
            "Epoch [400/1000], Loss: 0.0009\n",
            "Epoch [500/1000], Loss: 0.0002\n",
            "Epoch [600/1000], Loss: 0.0000\n",
            "Epoch [700/1000], Loss: 0.0000\n",
            "Epoch [800/1000], Loss: 0.0000\n",
            "Epoch [900/1000], Loss: 0.0000\n",
            "Epoch [1000/1000], Loss: 0.0000\n",
            "Model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "\n",
        "# Function to convert user input to a bag of words\n",
        "def bag_of_words(s, words):\n",
        "    bag = [0 for _ in range(len(words))]\n",
        "    s_words = nltk.word_tokenize(s)\n",
        "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
        "    for se in s_words:\n",
        "        for i, w in enumerate(words):\n",
        "            if w == se:\n",
        "                bag[i] = 1\n",
        "    return np.array(bag)\n",
        "\n",
        "# Chatting function\n",
        "def chat():\n",
        "    print(\"Start talking with the bot! (type 'quit' to stop)\")\n",
        "    while True:\n",
        "        inp = input(\"You: \")\n",
        "        if inp.lower() == \"quit\":\n",
        "            break\n",
        "\n",
        "        bow = bag_of_words(inp, words)\n",
        "        bow = torch.Tensor(bow).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            results = model(bow)\n",
        "        results_index = torch.argmax(results)\n",
        "        tag = labels[results_index.item()]\n",
        "\n",
        "        if torch.max(results).item() > 0.8:\n",
        "            for tg in data[\"intents\"]:\n",
        "                if tg['tag'] == tag:\n",
        "                    responses = tg['responses']\n",
        "            print(random.choice(responses))\n",
        "        else:\n",
        "            print(\"I don't understand. Please ask something else.\")\n",
        "\n",
        "# Start chatting\n",
        "chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iGt07z4D0Nq",
        "outputId": "a4428b61-a307-4502-ee86-d658d7bd5f0f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start talking with the bot! (type 'quit' to stop)\n",
            "You: Hi\n",
            "Good to see you again\n",
            "You: who are you\n",
            "I am Assist, a Deep-Learning chatbot.\n",
            "You: whats deep learning\n",
            "Deep learning differs from traditional machine learning in that it uses a hierarchical approach to learning, with multiple layers of artificial neurons, whereas traditional machine learning algorithms typically use simpler models.\n",
            "You: quit\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}